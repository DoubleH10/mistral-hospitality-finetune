{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwjZtcYP-ZRW"
   },
   "source": [
    "# Mistral-7B QLoRA — Hospitality Assistant (FAQs, Dialogs, Sentiment/Rewrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZrrIxv0CtA0"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQKWLv4zmaqE",
    "outputId": "04415734-714c-4a32-b987-4819a368fe04"
   },
   "outputs": [],
   "source": [
    "# Clone repo (or pull latest if already cloned)\n",
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/mistral-hospitality-finetune\"\n",
    "if os.path.isdir(REPO_DIR):\n",
    "    !git -C {REPO_DIR} pull\n",
    "else:\n",
    "    !git clone https://github.com/DoubleH10/mistral-hospitality-finetune.git {REPO_DIR}\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "# Install deps from pyproject.toml — single source of truth\n",
    "!pip install -q uv\n",
    "!uv pip install --system \".[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdTqjMx5xN7B",
    "outputId": "5c32c2be-f20a-4c10-e081-f7993d53fa53"
   },
   "outputs": [],
   "source": [
    "# Verify A100 GPU is connected (Colab Pro → Runtime → Change runtime type → A100)\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"No GPU detected! Change runtime: Runtime → Change runtime type → A100\"\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"GPU: {gpu_name} ({vram_gb:.0f} GB)\")\n",
    "assert \"A100\" in gpu_name, f\"Expected A100, got: {gpu_name}. Change runtime type to A100.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSFt9MF8Vauo",
    "outputId": "428dbbf4-bc6a-457d-8bb2-10c54432999c"
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(f\"bitsandbytes: {bnb.__version__}\")\n",
    "print(f\"torch: {torch.__version__}, CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cPXB6eXOAibp",
    "outputId": "3fde796c-5643-4daf-ffb5-e3bf1e3e2b20"
   },
   "outputs": [],
   "source": [
    "import os, torch\n",
    "\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "OUT_DIR = \"./outputs/mistral-hotel-qlora\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6306B6qPYTt"
   },
   "source": [
    "## 1) Load, Normalize & Split Datasets\n",
    "\n",
    "Each dataset is split independently **before** merging to guarantee zero train/val overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jZCqxU6QMuk"
   },
   "source": [
    "### SGD_Hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7No-h1-Eva8",
    "outputId": "06407ab5-86cc-451b-b817-087f887fa0c9"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds1 = load_dataset(\"vidhikatkoria/SGD_Hotels\")\n",
    "\n",
    "def clean_context(s, max_chars=1200):\n",
    "    \"\"\"Truncate dialog context from the top, keeping most-recent turns.\"\"\"\n",
    "    if not s: return \"\"\n",
    "    s = s.replace(\"<SEP>\", \"\\n\")\n",
    "    s = \"\\n\".join(line.strip() for line in s.splitlines() if line.strip())\n",
    "    if len(s) > max_chars:\n",
    "        s = s[-max_chars:]\n",
    "        if \"\\n\" in s:\n",
    "            s = s[s.index(\"\\n\") + 1:]\n",
    "    return s\n",
    "\n",
    "def sgd_to_text(ex):\n",
    "    # Only train on assistant turns (speaker == 1)\n",
    "    if ex.get(\"speaker\") != 1:\n",
    "        return {\"text\": None}\n",
    "    ctx = clean_context(ex.get(\"context\", \"\"))\n",
    "    resp = (ex.get(\"response\") or \"\").strip()\n",
    "    if not ctx or not resp:\n",
    "        return {\"text\": None}\n",
    "    instruction = \"Continue this hotel booking conversation as a helpful assistant.\"\n",
    "    user_msg = f\"{instruction}\\n\\nContext:\\n{ctx}\"\n",
    "    return {\"text\": f\"[INST] {user_msg} [/INST]{resp}</s>\"}\n",
    "\n",
    "sgd_all = ds1[\"train\"].map(sgd_to_text).filter(lambda ex: ex[\"text\"] is not None).shuffle(seed=42)\n",
    "\n",
    "# FIX Bug 1: proper train/val split — zero overlap\n",
    "sgd_split = sgd_all.train_test_split(test_size=0.15, seed=42)\n",
    "sgd_train, sgd_val = sgd_split[\"train\"], sgd_split[\"test\"]\n",
    "print(f\"SGD Hotels → Train: {len(sgd_train)}, Val: {len(sgd_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IptxHt_vCglE",
    "outputId": "de465c60-1f7d-4e73-aed6-e86a9dd8b0bd"
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(\"---\")\n",
    "    print(sgd_train[i][\"text\"][:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxR25qelQPq_"
   },
   "source": [
    "### Bitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQHPObLBPHy-",
    "outputId": "06a48f5f-b6e3-419a-fcf6-df0c98cafeea"
   },
   "outputs": [],
   "source": [
    "ds2 = load_dataset(\"bitext/Bitext-hospitality-llm-chatbot-training-dataset\")\n",
    "\n",
    "def bitext_to_text(ex):\n",
    "    inst = (ex.get(\"instruction\") or \"\").strip()\n",
    "    resp = (ex.get(\"response\") or \"\").strip()\n",
    "    if not inst or not resp:\n",
    "        return {\"text\": None}\n",
    "    intent = (ex.get(\"intent\") or \"\").strip()\n",
    "    cat = (ex.get(\"category\") or \"\").strip()\n",
    "    if intent or cat:\n",
    "        inst = f\"[{cat} | {intent}] {inst}\"\n",
    "    return {\"text\": f\"[INST] {inst} [/INST]{resp}</s>\"}\n",
    "\n",
    "bit_all = ds2[\"train\"].map(bitext_to_text).filter(lambda ex: ex[\"text\"] is not None).shuffle(seed=42)\n",
    "\n",
    "# FIX Bug 1: proper train/val split — zero overlap\n",
    "bit_split = bit_all.train_test_split(test_size=0.15, seed=42)\n",
    "bit_train, bit_val = bit_split[\"train\"], bit_split[\"test\"]\n",
    "print(f\"Bitext → Train: {len(bit_train)}, Val: {len(bit_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSmFe27APtvG",
    "outputId": "9d93ec1a-b39f-4cab-a440-db2041ab7648"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(\"---\")\n",
    "    print(bit_train[i][\"text\"][:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB2YXjy0QSXg"
   },
   "source": [
    "### Hotel Reviews — Dropped\n",
    "\n",
    "The `ashraq/hotel-reviews` dataset was evaluated but excluded from training:\n",
    "- **No gold sentiment labels** — the dataset only contains raw review text with no rating/sentiment column, so we would need to generate synthetic labels via heuristics or an external model.\n",
    "- **Hardcoded responses** — the v1 processing assigned identical canned responses (\"Mixed. Provide a one-sentence rationale.\") to every review regardless of content, meaning the model learned nothing useful.\n",
    "\n",
    "Keeping only SGD Hotels (dialog) and Bitext (FAQ) gives us two well-labeled, complementary task types with genuine training signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d74F5KMTP04b",
    "outputId": "26c256d9-f07b-4046-c187-47c150f6107a"
   },
   "outputs": [],
   "source": [
    "# (Hotel reviews dataset removed — see note above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb7_uf4fP-3K",
    "outputId": "cd98a09f-9dbd-4020-831b-48e2fc2f0aab"
   },
   "outputs": [],
   "source": [
    "# (Hotel reviews examples removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0u7ThYVQcjl"
   },
   "source": [
    "## 2) Merge & Verify No Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660,
     "referenced_widgets": [
      "3982255808f84d328f83ffdaf373121a",
      "cc219ba0a9f94baab55e7dc792530ac7",
      "f1f49cef901a4cf6969551319034be0a",
      "b17b31cd2e1b4e42a3ffb6d41fca276c",
      "7fad6d8dd9914db0b68a9deffc68d631",
      "487a166cca62417ba907ef10af5c8daf",
      "50a7de8c8c6d4c398e47900c2a5ec01e",
      "d15902ab95884bbba1e3decfafef5ce0",
      "040134f41d0a4068af17b056a79fe6a0",
      "9a91d38decf9492389d12a59ea4e139e",
      "310efdf36cf540918f2f5b62ea13acce",
      "cdbeb5c31f4545b0bd33ba8bdd6d6d5e",
      "070069db98db4e54bd66f6669a9f2b1d",
      "cdbc34c417ca4a38bfd62383b9b0514d",
      "08d48b1950b946a4aebaa757d50c642a",
      "1bdd3b6938224cd893aa41389e69989f",
      "725ebd9bfc6247189d85821dd3dd5fee",
      "3b6862977a6b4993b17a51042d9c66ea",
      "10fecc30575140e88733e90185de6ea7",
      "c74fcf1c629b4c6f839f4899918dcb13",
      "d39d3a9d2f394fb993e2ae332282f4e7",
      "117c76c7b78044c09b61abfb138987d6",
      "c9e9439428e947b69cc03c6356ea9f5f",
      "1473062f19594d3d87143ba3648e20a9",
      "e5eeabc4822b4fc291a2124368e8fc1b",
      "f90cfa101c1442c2b05d13524ea3b756",
      "146e854c020b4ed88fb885294dbef828",
      "d02add21bd674d3ab6bc17f512075426",
      "7b2fe1c003cc47bd8268568b6debf6d8",
      "1c58f92a89f742b99e18f3f913aa2c23",
      "9034c73274724972a636a55cf0eed76c",
      "5a417272a65f45afa87340e0259b2bc3",
      "8b2cd709ce8b4c18a3544b5c44b9baf4",
      "4ddc7bb68cf240cb8efdeaf0b809ab41",
      "7e89bc6149234bcc8c97c7dba319470c",
      "06e2bd81008d4b15bd4eaef7d085bb4b",
      "a96968577e13436ca2b1e73d5d57fde6",
      "ea041158e1e64d71b1901fb2e6c8d374",
      "33410fd178794b5cb41516d551fec536",
      "95c5056c0a9546cf97e71542d5f8fe9d",
      "c815abc5e92f4d85ad29e9c11b4e796e",
      "77e191bb1d3d481dbd26f472a313d7f3",
      "5727bb434f1f48bc89f3ef98f28b6a56",
      "0d415cf4252147cf9a6b1d04a280d34b",
      "55b510e2ee8849748d1f708f119951b0",
      "d1bd28f5382d4ed59dac54cb4fa4a013",
      "f0a3cd07e185416785113eb75cd0b85e",
      "04c9889d904d4b1daa8b50253cf933e2",
      "3b874d53cb3c4a3bb9e871c7e576eb8f",
      "18e0b6037bc14dbb86bed4920fda2661",
      "74006fba829b4cc4b728b540cc7f3aa6",
      "66e325a7255a403cafce30d17f3a9960",
      "33dbbffc8b7f4741bcc7871885f32064",
      "c79ae50926ee424499a2e618ca39c0e1",
      "602811a272b64eb6b281357d479c7f66",
      "0004e9dd3295478ba43ae86505e56593",
      "394bd8217269421998e02261e2c20c2c",
      "8a5a369d4c544f59bea640869a268232",
      "34bd77b94dfe49f4be0c62fc019f317a",
      "58ec71be43854c9e805f1ddff9f8f574",
      "f6e461583edd484f97913f5e85735fb2",
      "349949475d894943bd76ac40046390b7",
      "2e28089d648b4fa28c9eb2f693e8cef7",
      "7cdc3e08ac4e42ecb62404befd3202df",
      "71b5229617a34c258d1073b064a99954",
      "a4975b2fc37c45768c61497204fcd864"
     ]
    },
    "id": "JBf9PYUjQbMm",
    "outputId": "09e504d5-f4dc-49f4-cfb8-f9a6599a80cd"
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Merge the independently-split datasets\n",
    "merged_train = concatenate_datasets([sgd_train, bit_train]).shuffle(seed=7)\n",
    "merged_val = concatenate_datasets([sgd_val, bit_val]).shuffle(seed=11)\n",
    "\n",
    "# Cap total size for a manageable Colab run\n",
    "MAX_TRAIN, MAX_VAL = 2500, 500\n",
    "if len(merged_train) > MAX_TRAIN:\n",
    "    merged_train = merged_train.select(range(MAX_TRAIN))\n",
    "if len(merged_val) > MAX_VAL:\n",
    "    merged_val = merged_val.select(range(MAX_VAL))\n",
    "\n",
    "# Verify zero overlap between train and val\n",
    "train_texts = set(merged_train[\"text\"])\n",
    "val_texts = set(merged_val[\"text\"])\n",
    "overlap = train_texts & val_texts\n",
    "assert len(overlap) == 0, f\"DATA LEAKAGE: {len(overlap)} examples in both train and val!\"\n",
    "print(f\"Train: {len(merged_train)}, Val: {len(merged_val)}, Overlap: {len(overlap)}\")\n",
    "\n",
    "ds = DatasetDict({\"train\": merged_train, \"validation\": merged_val})\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "# Preview a formatted example\n",
    "print(\"\\n--- Sample training example ---\")\n",
    "print(ds[\"train\"][0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhGR87bTTXVB"
   },
   "source": [
    "## 3) QLoRA + SFTTrainer (completion-only loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907,
     "referenced_widgets": [
      "eba7c76c65f042ffb0190e9b4f31735f",
      "00ca82f972b44396a4a942653d67e402",
      "8d3646847fc94716ac87896351a1fb70",
      "7cf924fe32074ae28acb8cd354158ef1",
      "fa13c624cc024a43ad7ea6d28f534d0d",
      "625ba253a4cf480d8d7bb9e901e685a6",
      "31743e64f9b646468efcbbc85636824f",
      "157c96fbcc66423999a478c51a35d38b",
      "de9070b577b341008bee87207f2cd44c",
      "b835d5a399ce4215a597a374612d4f0b",
      "021f3c9cc48d4c0588583ed94812ac95",
      "36f0b3d7f7bf4ee6917fd6dbe88246c4",
      "7d61612c5bc24f23952f07dd234d5d8f",
      "2e1f28c5391f4b51b0a7280bf632896a",
      "5647cc6ac4c047e7b453caa4bc58acb9",
      "9f7f3b5c21514585acfd9aaea1f9124a",
      "1aa9abea3e66432eadf5683a4b94c130",
      "d37780107cc94ad2b1049e77d6b63abc",
      "61b57ba33d0d4a0ba8fdcf00c484ede8",
      "7e9dd199485741f6a3c91824e9af3894",
      "aa40c518017d4c299a232b58e3e11b0e",
      "fe74e2f8ef794a58bd9e616a8ccbb1ab",
      "af52f19e3809464b916d447bb028d015",
      "5abf719daa4e4cc0b88a2cfac47e31ff",
      "8b71e49240fe473d98968b9bb6eb435b",
      "c21e898939824b86a01c08972a5e493f",
      "30f36af6e3fe454498ed242f1377705b",
      "e4e26cd03dc14025865e4166ee446427",
      "c19c78c382834b139ab66189937960b6",
      "c4e396c8d2084c0ab73bf58b3f3af908",
      "77176266cfb94215abd44f8d9807e749",
      "f1d76728118449078c703415b9804581",
      "b5b5f537e97e4322af0a9939e4927487",
      "d0ecf88b4742407b8e9f38c6ff25e856",
      "8107837ab68e49c7a5991abff74f34c3",
      "d1e42d0266d14ce386c75006f1085cb7",
      "6c73401678244688ba268d55e55bb4e5",
      "683c20f4bc18422dabb7377451a4b97e",
      "225c68f1756a4477a2f737ba7c2cbda8",
      "41b2e1cd13a34218907e64c2e514547f",
      "f39b6cc5c5d74326bf1c0b812d5bf37a",
      "004afc20c57741beb26406d07a511a05",
      "a179dfdb94904e3d9d8541ad45abe5c0",
      "72c9d93593c4442380847aef17cf4160",
      "d714626933f5408fbed8c196ffea24fb",
      "01e91b4145a042dc8d6489b94e8f6b06",
      "af8315f3215e48bf836133660fa34972",
      "4bea8fe20c2e4de2af7d457830a9aa53",
      "8756356d7a124910a5dfa741f36e011a",
      "0169e1fa41e24760ad91e1479b38d5a8",
      "159d979783f54efd89ea965ad6bb7ffa",
      "eb9571d3f99a4e9bb347eaaef6ae3ace",
      "52debf4105f6418fb9603f5f7f848507",
      "f7a610513710403280aced777fab202a",
      "e490f640749040af942b2223a7a5a454",
      "e15f8c5ce8a4447ea0d25d6d2e10ea8b",
      "2d78ab49e4f94a74bd88d26c78ddee03",
      "7082bb7415024d0295f4020f3ce0c237",
      "14eb46445703410099fd7b69b318dcf5",
      "3ac2d9c20da74383ac7c3df57eedbd2b",
      "6ff4d6679e9440d79bbe59e8fdef6e80",
      "6daad307900a48cfafede0cdd04b4d34",
      "dd5e1f43aa574fb7a374302c2cb8c97d",
      "77e7ceb7cbc544bebc06615f5704ad36",
      "f457e949ac4c40bc90daeea6101bd2a4",
      "4b692b12cfb14ffe9cfddb76aefad366",
      "d4eff8cf1f454386b7323674edcc7860",
      "a77ca494df644a62bd371da2d4ac0574",
      "83edb46f8e5a468d8191d37613bae978",
      "fe3200802772489fae8bc54f05591a38",
      "ac70ad7feb4b4dbcb5a0c73f0c80b9e0",
      "71f46557692c4874a812ef75dd6e5db8",
      "bc19a209bd12403cb5a82c6cd0ee0a94",
      "52775f003b0c4db39e274bb2fdad1203",
      "bbdd21912acb44d4b762e8584284bf60",
      "211ec80ff8bd4850b3da1ec94c6639ce",
      "e6427d1db9b248e2abfbe3d6b9fe5276",
      "fa341500126041aaadedc44bd171ccee",
      "207464bbaeb4423eb1821098308bfc25",
      "dbd6363450a64ccba77305920c2c9f91",
      "745eb654492f49c59692429b3eb657c2",
      "98172cfad8884e868d87e1fedcb52f78",
      "bfd728483ebd401d911b36cb1e606b65",
      "769cac984ca54c0195f63f35955d1cbf",
      "72e9cddaf7e540ea88b2580567c25185",
      "93de8d616a8d41f68977b89c3b19c19d",
      "c16a23349f594ba1b39f5162033a144c",
      "2a76cd1be2224ff1893d3e7b37a5a27f"
     ]
    },
    "id": "9sgJ7WSgUMaN",
    "outputId": "1a4b27c6-5572-4eae-9409-3e84d7b25f57"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# --- QLoRA quantization config ---\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# --- Load base model (4-bit) ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.gradient_checkpointing_enable(\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# --- LoRA adapters ---\n",
    "peft_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- FIX Bug 2: completion-only loss ---\n",
    "# Loss is computed ONLY on tokens after [/INST] (the response).\n",
    "# This avoids inflating metrics by predicting instruction tokens.\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=\"[/INST]\",\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "# --- Training args ---\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,   # effective batch size = 8\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    logging_steps=25,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# --- SFTTrainer ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTV5pPraiZrR"
   },
   "source": [
    "## 4) Before / After Comparison\n",
    "\n",
    "Both base and adapter are tested with the **same** `[INST]...[/INST]` template so the comparison is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ab3ffb2605964a8eacc3f2d71ea7f2ce",
      "13960d9f845b4b51b5afa3ba45acb064",
      "87ff7cb4bbb24f86a4152ad711485c46",
      "7faf6937f2d74b8bacdec31c76401a17",
      "0083c9e366884ef7aa69078a19e6f60f",
      "f190c84f4dda4d9283f14e5af6332f50",
      "880cdca1f44e4426836b41eb7aa35d73",
      "dd1fabc09ef5437fa0bacced8ae9e41b",
      "eb80e800760548b7a0f73f63279492ac",
      "84b424b9133942b68098d017313e4967",
      "84d4ae11d21940fb833fda45bd31b55f"
     ]
    },
    "id": "nY6ov0rBTVXQ",
    "outputId": "25a1683c-0f8c-406d-c09a-c9a874ceb3ed"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load fresh base + adapter for fair comparison\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, quantization_config=bnb_cfg, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "adapted = PeftModel.from_pretrained(base_model, OUT_DIR).eval()\n",
    "\n",
    "def generate(model, prompt, max_new_tokens=160):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    # Extract response: everything after [/INST]\n",
    "    if \"[/INST]\" in text:\n",
    "        return text.split(\"[/INST]\")[-1].strip()\n",
    "    return text[len(prompt):].strip()\n",
    "\n",
    "# FIX Bug 3: All prompts use correct [INST]...[/INST] template\n",
    "tests = [\n",
    "    # SGD-style dialog\n",
    "    \"[INST] Continue this hotel booking conversation as a helpful assistant.\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    \"User: I need a room in Rome for 2 nights next week.\\n\"\n",
    "    \"Assistant: Certainly. What dates are you arriving and leaving?\\n\"\n",
    "    \"User: Arrive 12 May, leave 14 May. Prefer near Termini. [/INST]\",\n",
    "\n",
    "    # Bitext-style FAQ\n",
    "    \"[INST] What are the check-in and check-out times? [/INST]\",\n",
    "\n",
    "    # Intent-tagged FAQ\n",
    "    \"[INST] [BILLING | invoices] Where can I find my invoices? [/INST]\",\n",
    "\n",
    "    # Booking modification\n",
    "    \"[INST] [BOOKING | modify_booking] I want to change my reservation to a different date. [/INST]\",\n",
    "]\n",
    "\n",
    "for prompt in tests:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"PROMPT: {prompt[:100]}...\")\n",
    "    # Temporarily detach adapter for base comparison\n",
    "    adapted.disable_adapter_layers()\n",
    "    print(f\"\\nBASE:    {generate(adapted, prompt)}\")\n",
    "    adapted.enable_adapter_layers()\n",
    "    print(f\"ADAPTER: {generate(adapted, prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "QHmdiZgW3Epq",
    "outputId": "d8713bf2-b382-4c7f-d594-b2d43c0ca3eb"
   },
   "outputs": [],
   "source": [
    "import math, json\n",
    "\n",
    "# Final evaluation\n",
    "eval_metrics = trainer.evaluate()\n",
    "ppl = math.exp(eval_metrics[\"eval_loss\"])\n",
    "print(f\"Eval loss (completion-only): {eval_metrics['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n",
    "\n",
    "# Save metrics to outputs/ for reproducibility\n",
    "results = {\n",
    "    \"eval_loss\": eval_metrics[\"eval_loss\"],\n",
    "    \"perplexity\": round(ppl, 2),\n",
    "    \"train_samples\": len(ds[\"train\"]),\n",
    "    \"val_samples\": len(ds[\"validation\"]),\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"completion_only_loss\": True,\n",
    "    \"data_leakage_check\": \"passed\",\n",
    "}\n",
    "metrics_path = os.path.join(OUT_DIR, \"metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\nMetrics saved to {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
